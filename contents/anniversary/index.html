
<!DOCTYPE html>
<html lang='en' class=''>

<head>

  <meta charset='UTF-8'>
  <title>VOT Challenge 10 years anniversary</title>

  <meta name="robots" content="noindex">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700">

<link rel="stylesheet" href="./style.css">

</head>

<body>
  <body data-spy="scroll" data-target="#navbar-example">


  <div class="wrapper" >

    <nav class="nav__wrapper" id="navbar-example">
      <img class="logo" src="logo.png" />

      <ul class="nav">

        <li role="presentation" class="active">
          <a href="#section0">
            <span class="nav__title">Intro</span>
          </a>
        </li>

        <li role="presentation">
          <a href="#section1"> 
            <span class="nav__title">2013</span>
          </a>
        </li>

        <li role="presentation">
          <a href="#section2">
            <span class="nav__title">2014</span> 
          </a>
        </li>
         
        <li role="presentation">
          <a href="#section3"> 
            <span class="nav__title">2015</span>
          </a>
        </li>
        
        <li role="presentation">
          <a href="#section4">
            <span class="nav__title">2016</span>
          </a>
        </li>
        
        <li role="presentation">
          <a href="#section5">
            <span class="nav__title">2017</span>
          </a>
        </li>

        <li role="presentation">
            <a href="#section6">
              <span class="nav__title">2018</span>
            </a>
          </li>

          <li role="presentation">
            <a href="#section7">
              <span class="nav__title">2019</span>
            </a>
          </li>

          <li role="presentation">
            <a href="#section8">
              <span class="nav__title">2020</span>
            </a>
          </li>

          <li role="presentation">
            <a href="#section9">
              <span class="nav__title">2021</span>
            </a>
          </li>


          <li role="presentation">
            <a href="#section10">
              <span class="nav__title">2022</span>
            </a>
          </li>

          <li role="presentation">
            <a href="#section11">
              <span class="nav__title">People</span>
            </a>
          </li>

          <li role="presentation">
            <a href="#section12">
              <span class="nav__title">Future</span> 
            </a>
          </li>

      </ul>
    </nav>

    <section class="section section1" id="section0">
        <h1>Welcome to the online exhibition celebrating 10 years of the Visual Object Tracking Challenge!</h1>

        <p>Visual object tracking is a crucial task in <b>computer vision</b>, with applications ranging from video surveillance and driver assistance systems, to augmented and virtual reality. 
        Over the past decade, the Visual Object Tracking Challenge (VOT) has become a major benchmark for the <b>evaluation</b> of object tracking algorithms, 
        attracting the participation of researchers from around the world.<br/><br/>
        
        This short exhibition showcases the <b>evolution</b> of object tracking techniques through the perspective of <b>VOT workshops</b>. 
        We hope that this exhibition will provide an <b>informative</b> experience for all visitors, 
        and <b>inspire</b> you to continue pushing the boundaries of <b>visual object tracking research</b>.</p>

        <br/><br/><br/>
        <p>
            <span class="arrow pulse"></span>
            <span class="arrow pulse"></span>    
            <span class="arrow pulse"></span>    
        </p>
        
    </section>

    <section class="section section1" id="section1">
        <h1>In the beginning there was a methodology ...</h1>

        <p>The VOT initiative started from the apparent <b>lack of standardization</b> of evaluation in the field of <b>visual object tracking</b>. 
            It started with a paper on <b>evaluation methodology</b> for visual object tracking that attracted attention of researchers at a local workshop (CVWW). 
            The first challenge was presented at a workshop at the <b>International Conference on Computer Vision</b> (ICCV) in 2013, the conference was held in Sydney, Australia.</p>
        
        <div class="gallery">
            <img src="2013/1.jpg" alt="Matej Kristan is presenting the results" />
            <img src="2013/2.jpg" alt="Working lunch" />
            <img src="2013/3.jpg" alt="Invited lecture by Mubarak Shah" />
            <img src="2013/4.jpg" alt="Panel discussion" />
        </div>

        <p>The initial challenge used a single RGB dataset of <b>16 sequences</b> annotated with axis-aligned bounding boxes as it was standard in tracking community at the time. Sequences we also annotated with <b>per-frame attributes</b>.
            
          The challenge received <b>27</b> submissions by <b>59 authors</b> from <b>ten countries</b>. 
          The participants used a dedicated evaluation toolkit developed for the challenge to download the dataset and run the experiments. Results were then sent to the organizing committee.</p>
    </section>
 
    <section class="section section2" id="section2">
        <h1>Rolling on and gaining momentum</h1>

        <p>After the success of the first VOT challenge, it became clear that this may become a recurring event. The second VOT challenge refreshed the dataset with <b>additional sequences</b> and introduced the first attempt to improve limitations of axis-aligned bounding boxes by introducing <b>rotated bounding boxes</b>. The evaluation toolkit supported a new integration protocol called <b>TraX</b> that offered more flexible experimental setups and faster execution. The results of the challenge were presented at a workshop at the <b>European Conference in Computer Vision</b> (ECCV) in 2014 in Zurich, Switzerland.</p>
        <div class="gallery">
            <img src="2014/1.jpg" alt="Results presentation" />
            <img src="2014/2.jpg" alt="Winner ceremony" />
            <img src="2014/3.jpg" alt="Organizers group photo" />
            <img src="2014/4.jpg" alt="Working dinner after the workshop" />
        </div>

        <p>The challenge received <b>36</b> submissions by <b>65 authors</b> from <b>ten countries</b>. In contrast to the first challenge, where no specific methodology was trending, the second challenge saw the raise of trackers based on <b>correlation filters</b>. These trackers were very fast and versatile. </p>
    </section> 

    <section class="section section3" id="section3">
        <h1>Deep learning meets visual tracking</h1>

        <p>The third challenge introduced a <b>new evaluation measure</b> that combined the two aspects, benchmarked in previous challenges: <b>robustness</b> and <b>accuracy</b>. The toolkit was upgraded to support this new evaluation. The dataset was again refreshed, its size increased to <b>60 sequences</b>. A new sub-challenge based on <b>thermal imaging</b> was introduced to promote the specific nature of different data modalities. The results of the challenge were presented at a workshop at the <b>International Conference in Computer Vision</b> (ICCV) 2015 in Santiago, Chile.
        </p>

        <div class="gallery">
          <img src="2015/1.jpg" alt="Audience listening to worksop opening" />
          <img src="2015/2.jpg" alt="Matej Kristan is presenting the results" />
          <img src="2015/3.jpg" alt="Demo session: Jae-Yeong Lee presenting Umshift tracker" />
          <img src="2015/4.jpg" alt="Organizers group photo" />
        </div>

        <p>The challenge received <b>86</b> submissions by <b>155 authors</b> from <b>14 countries</b>. A large part of the submissions utilized correlation filters, but the winning tracker was already using <b>deep learning</b> detector methodology. This approach enabled more <b>robust detection</b> at the cost of much <b>slower</b> runtime.</p>
    </section>


    <section class="section section4" id="section4">
        <h1>Consolidation and reannotation</h1>

        <p>The fourth challenge was primarily used to <b>consolidate</b> the dataset for short-term RGB tracking. The dataset was <b>re-annotated</b> with a special procedure that involved <b>segmentation</b>. This then enabled more objective bounding box fitting. The thermal sub-challenge was also organized. The results of the challenge were presented at a workshop at the <b>European Conference in Computer Vision</b> (ECCV) in 2016 in Amsterdam, Netherlands.</p>

        <div class="gallery">
          <img src="2016/1.jpg" alt="Workshop audience" />
          <img src="2016/2.jpg" alt="Panel discussion" />
          <img src="2016/3.jpg" alt="Winner awards ceremony" />
          <img src="2016/4.jpg" alt="Organizers group photo" />
        </div>

        <p>The challenge received <b>92</b> submissions by <b>169 authors</b> from <b>18 countries</b>. The primary methodologies remained <b>deep learning</b> and <b>correlation filters</b> that received performance boost by using more descriptive (deep) <b>features</b> to work with.</p>
    </section>

    <section class="section section5" id="section5">
        <h1>Measuring real-time</h1>

        <p>Starting with the fifth VOT challenge the <b>TraX protocol</b> that was optional before became the only possible way of integrating trackers with the toolkit. This enabled a more realistic <b>real-time experiment</b> with frame drops. The dataset was again refreshed and a <b>sequestered dataset</b> was introduced for final winner identification to prevent overfitting of methods to the dataset. The results of the challenge were presented at a workshop at the <b>International Conference in Computer Vision</b> (ICCV) 2017 in Venice, Italy.</p>

        <div class="gallery">
          <img src="2017/1.jpg" alt="Matej Kristan and Jiri Matas are presenting the results" />
          <img src="2017/2.jpg" alt="Invited talk" />
          <img src="2017/3.jpg" alt="Workshop audience" />
          <img src="2017/4.jpg" alt="Winner award ceremony" />
          <img src="2017/5.jpg" alt="Erhan and Aydin with their winner certificate" />
        </div>

        <p>The challenge received <b>52</b> submissions by <b>123 authors</b> from <b>14 countries</b>. A more comprex integration as well as running only a single challenge resulted in a slight drop in submission number. 
          On the other handled a proper real-time experiment highlited the performance benefits of ceratin methods, espectially <b>correlation</b> filters and <b>Siamese</b> deep models.
        </p>
    </section>

    <section class="section section6" id="section6">
      <h1>Evaluating long-term tracking</h1>

      <p>The sixth challenge introduced a <b>long-term tracking</b> category that was again based on prior research work on evaluation methodology. In the case of long-term tracker re-detections are very important and this required some changes in the evaluation toolkit and a new dataset that contained many <b>occlusions</b> and <b>out-of-view</b> conditions. The results of the challenge were presented at a workshop at the <b>European Conference in Computer Vision</b> (ECCV) in 2018 in Munich, Germany.</p>

      <div class="gallery">
        <img src="2018/1.jpg" alt="Workshop audience listening to results announcement" />
        <img src="2018/2.jpg" alt="Panel discussion" />
        <img src="2018/3.jpg" alt="Invited talk" />
        <img src="2018/4.jpg" alt="Conference dinner" />
      </div>

       <p>The challenge received <b>87</b> submissions by <b>195 authors</b> from <b>16 countries</b>. In contrast to previous years, a wider use of <b>localization-trained</b> deep features and deep trackers based on <b>Siamese</b> architectures was observed in addition to <b>bounding box regression</b>. These architectures enabled <b>larger search windows</b> and could thus locate faster targets. In terms of long-term tracking, the use of <b>redetection</b> became apparent.
       </p>
      </section>

    <section class="section section7" id="section7">
      <h1>Going multi-modal</h1> 
        
      <p>The seventh VOT challenge expanded the list of sub-challenges with <b>multi-modal</b> sequences. Two new sub-challenges were introduced next to the RGB short-term and long-term challenges that received dataset updates. The thermal sub-challenge that was suspended after two years was revived as <b>RGBT</b> sub-challenge and a <b>RGBD</b> sub-challenge was added to facilitate the increasing number of depth cameras. The toolkit and the TraX protocol were extended to support multi-channel sequence frames. The results of the challenge were presented at a workshop at the <b>International Conference in Computer Vision</b> (ICCV) 2019 in Seoul, South Korea.
      </p>

        <div class="gallery">
          <img src="2019/1.jpg" alt="Aleš Leonardis introducing the challenge" />
          <img src="2019/2.jpg" alt="Winners presentation" />
          <img src="2019/3.jpg" alt="Winners presentation" />
          <img src="2019/4.jpg" alt="Organizing committee group photo" />
        </div>

        <p>
          The challenge received <b>81</b> submissions by <b>219 authors</b> from <b>17 countries</b>. More methods used deep learning in some way, some also combined multiple <b>pre-trained models</b> to improve tracking. <b>Semantic segmentation</b> is used
          to improve accuracy of top trackers making clear that the field of visual tracking is ready for more accurate and specific groundtruth.
        </p>
    </section>

    <section class="section section8" id="section8">
      <h1>Tracking with segmentation</h1>

      <p>The eight challenge introduced a new type of annotation in the form of <b>segmentation masks</b>. The idea was to push forward the accuracy by enabling trackers to report very <b>detailed positions</b> of objects. This change required minor changes to the communication protocol. A <b>new evaluation toolkit</b> also replaced the previous one, it was implemented in Python> that became the dominant programming language in computer vision research. Similarly to the previous year, four sub-challenges were run, <b>short-term</b>, <b>long-term</b>, <b>RGBD</b>, and <b>RGBT</b>. The results of the challenge were presented at a virtual workshop at the <b>European Conference in Computer Vision</b> (ECCV) 2020.</p>

      <div class="gallery">
        <img src="2020/1.jpg" alt="Results presentation" />
        <img src="2020/2.jpg" alt="Winners talk" />
        <img src="2020/3.jpg" alt="Organizing team credit slide" />
        <img src="2020/4.jpg" alt="Panel discussion" />
      </div>

      <p>The challenge received <b>58</b> submissions by <b>131 authors</b> from <b>14 countries</b>. The shift towards segmentation in the main sub-challenge and the pandemic uncertainty have decreased the number of participants. Despite that, the submissions were interesting and while true segmentation trackers were not common, many methods were still adapted to the new scenario using specialized single-short segmentation heads that were appended to the main tracker architecture.</p>
    </section>

    <section class="section section9" id="section9">
      <h1>The rise of transformers</h1>
        <p>The ninth VOT challenge was again about <b>consolidation</b>. The core RGB short-term dataset was updated according to the established method of removing easy sequences and introducing sequences that <b>increased diversity</b>. The long-term and RGBD sub-challenges were also organized with no changes to their datasets while the RGBT sub-challenge was not. The results of the challenge were presented at a virtual workshop at the <b>International Conference in Computer Vision</b> (ICCV) 2021.</p>

        <div class="gallery">
          <img src="2021/1.jpg" alt="Results present" />
          <img src="2021/2.jpg" alt="Invited talk" />
          <img src="2021/3.jpg" alt="Organizing team credit slide" />
          <img src="2021/4.jpg" alt="Panel discussion" />
          <img src="2021/5.jpg" alt="Mateo and Christian with their winner certificate" />
        </div>

        <p>The challenge received <b>71</b> submissions by <b>157 authors</b> from <b>15 countries</b>. From the methodological perspective many well performing trackers used attention blocks, popularized by <b>transformer architectures</b>. These methods handled large motions and appearance changes and were also fast enough to be used in real-time.
        </p>
    </section>

    <section class="section section10" id="section10">
      <h1>Towards the tracking multiverse</h1>

      <p>The tenth VOT challenge introduced the <b>highest number of sub-challenges</b> since its beginning, the <b>short-term</b> challenge was evaluated on segmentation and aligned-bounding box trackers, both challenges also included <b>realtime</b> counterparts. The short-term dataset was refreshed with new sequences. The <b>RGBD</b> dataset was used for RGBD and Depth challenges, both now using short-term evaluation methodology. The <b>long-term</b> sub-challenge introduced an entirely <b>new dataset</b>. The results of the challenge were presented at a workshop at the <b>International Conference in Computer Vision</b> (ICCV) 2022 in Tel Aviv, Israel.</p>

      <div class="gallery">
        <img src="2022/1.jpg" alt="Matej Kristan is presenting the results" />
        <img src="2022/2.jpg" alt="Matej Kristan is presenting the results" />
        <img src="2022/3.jpg" alt="Matej Kristan is presenting the results" />
        <img src="2022/4.jpg" alt="Matej Kristan is presenting the results" />
      </div>

    <p>The challenge received <b>93</b> submissions by <b>180 authors</b> from <b>11 countries</b>. After two years of virtual workshops that have notably reduced interest in this kind of event, this year the workshop was a hybrid event. The interest in visual tracking has again been gaining momentum. At the same time the research in visual object tracking is increasingly <b>interacting</b> with other research fields like <b>detection</b> and <b>segmentation</b>. These kinds of interactions will likely shape the convergence of methods in the future.</p>

    </section>


    <section class="section section11" id="section11">
        <h1>Community testimonials</h1>
        <p>In the span of a decade the VOT challenges impacted carreers of many researchers. We have reached to them to get some feedback. If you would like to add your own thoughts you can still contact us.</p>
    
        <div class="gallery testimonials">
          <p>
            Datasets, tutorials and source codes from the VOT Challenge portal are a great help for students in the course "Artificial Intelligence Systems" that I teach and projects. I even hold competitions among students and they try to improve the results of the winners of the VOT challenge.
            The VOT initiative team is amazing! Congratulations! I wish you great success! 
            
            <span class="author">Aleksandr Romanov, HSE University</span>
          </p>

          <p>
            We - my PhD advisor and myself - joined this great community and challenges from 2016 until 2018. VOT Challenge motivated us as researchers who are curious about visual tracking because it increases the visibility of innovative and working solutions, and provides a unique benchmarking platform that has continuously raised the bar on scientific evaluation and brought a novel perspective to evaluating short- and long-term tracking. Following the challenge every year helped us zoom out and understood what was needed to go beyond the boundaries of the state-of-the-art family of trackers. Moreover, being able to access data and tools that the VOT community provided was easy and handy to ablate approaches. Joining the challenge and becoming a winning team is a great experience that connected us with enthusiasts in the field. We are grateful to this meticulous organization for making our research more visible to a large extent.

            <span class="author">Erhan and Aydin</span>

          </p>
          <p>
            The VOT workshop had an important influence on shaping my PhD path, and consequently it showed me a model of how to conduct research in visual tracking. During the work towards my PhD thesis, my group and I targeted the VOT workshop as a major event to submit papers and discuss ideas. I had the privilege of presenting a paper at the workshop in each of the three years of my PhD. VOTs also hosted significant moments of my career, of which I attach two pictures: my first conference presentation of my very first first-author paper, happened at VOT2019; and my first research award, at the (unfortunately virtual) VOT2021. I am grateful to the organizers for giving me the opportunity of living such experiences. Our confidence in the event and in the expertise of the organizing committee was not just because of the VOT challenge’s popularity in computer vision, but also because we always received detailed and useful feedback from the reviewers and participants, even better than the one got from the main conference’s reviewers. 

          I wish you (at least) other 10 successful editions of the VOT challenge!

          <span class="author">Matteo Dunnhofer, University of Udine</span>
          </p>
          <p>
            There is no doubt that the VOT Challenge has continued to push the field of tracking through thoughtful organizations over the past decade. In the 10th VOT Challenge in 2022, I am very excited to see that our AOT algorithm based on segmentation tracking achieves a robustness beyond the bounding-box tracking algorithms for the first time. I believe this breakthrough will promote another development in the tracking field.

            <span class="author">Zongxin Yang</span>
          </p>
          <p>
            Anyone who's tried to read and compare tracking papers from before 2013 will immediately appreciate the importance of the VOT challenge. The existence of a standard, representative benchmark has been critical in enabling the rapid, positive progress of the field. While it's important to remember that benchmarks are primarily a means for evaluating scientific hypotheses, it's undeniable that healthy competition has also helped drive the advancement of object tracking over the past decade. The people behind the challenge have been reliable, selfless and indefatigable over the years, not only organising the annual competition, workshop and report, but also working to continuously improve the challenge in response to the needs of the community. They must be congratulated and thanked for this mammoth effort that has benefited the field as a whole.

            <span class="author">Jack Valmadre, The University of Adelaide</span>
          </p>
          </div>
    
    
    </section>

    <section class="section section12" id="section12">
        <h1>And now what?</h1>

        <p>
          In 2022 the VOT Initiative celebrated 10 years of continuous effort to push forward visual object tracking research. 
          The organizing committee tried to listen to trends in the community as well as to shape the progress of the research. 
          It is clear that it is now time for some major changes.

          A decade is a long time in computer vision, as some questions are answered and new ones appear. Research directions that 
          evolved independently for long time, suddenly start converging. 
          It looks like something like this may happen to visual tracking and the broder field of motion understanding as well.
        </p>
    </section>

  </div>

  <div id="modal" class="modal">
    <img class="modal-content" id="image">
    <div id="caption"></div>
  </div>

</body>
  
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<script>
// SMOOTH SCROLLING SECTIONS
$('a[href*=#]:not([href=#])').click(function() {
    if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'') 
        || location.hostname == this.hostname) {

        var target = $(this.hash);
        target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
           if (target.length) {
             $('html,body').animate({
                 scrollTop: target.offset().top
            }, 1000);
            return false;
        }
    }
});

$("#modal").hide();

$("#modal").click(function() { $("#modal").hide(); });

$(".gallery img").click(function() {
  $("#image").attr("src", $(this).attr("src"));
  $("#caption").text($(this).attr("alt"));
  $("#modal").show();
});
/*
  .each(function (index, elem) {
  var title = elem.attr("alt");

  var t

  elem.replaceWith()


});
*/

</script>
</body>

</html>